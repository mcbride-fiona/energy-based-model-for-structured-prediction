# Image-to-Text with Energy-Based Models

This project implements and trains an **energy-based model (EBM)** for structured sequence decoding tasks using synthetic word images. It demonstrates how EBMs can be used to infer discrete structured outputsâ€”like character sequencesâ€”from high-dimensional inputs such as images, using differentiable energy functions and dynamic programming. Instead of predicting characters with a softmax at each time step, we learn an energy landscape over latent â€œwindow Ã— classâ€ decisions, and then use dynamic programming (DP) to find low-energy alignments and to decode full words.

---

## ğŸ” Project Overview

- **Model**: A simple CNN-based encoder trained to minimize energy along the correct label path, using a custom loss.
- **Data**: Synthetic images of lowercase words generated via `SimpleWordsDataset`.
- **Training**: Supervised learning with cross-entropy loss over optimal alignment paths.
- **Inference**: Recovers the most likely character sequence by decoding energy maps using dynamic programming.
- **Visualization**: Path matrix and energy heatmaps are saved as images.

This project was originally inspired by a homework assignment but expanded into a general-purpose EBM pipeline for experimenting with structured decoding.

---

## Quick Start for MacOS

### 1. Clone the repository
git clone https://github.com/your-username/energy-based-model-for-structured-prediction.git

cd energy-based-model-for-structured-prediction

### 2. Set up your environment
python -m venv venv

source venv/bin/activate

pip install -r requirements.txt

### 3. Run training (not necessary)
python main.py --epochs 1 --batch-size 32

### 4. Run inference
python main.py --skip-train --checkpoint outputs/checkpoints/best.pth --word "your-words"

---
```bash
Repository Structure
â”œâ”€â”€ main.py           # Entry point: trains (N epochs), logs char acc, then decodes --word
â”œâ”€â”€ train.py          # Trainer + DP utilities + decoder + logger + config snapshot
â”‚   â”œâ”€â”€ train_ebm_model()     # One-epoch training loop (backprop on DP-aligned CE)
â”‚   â”œâ”€â”€ build_ce_matrix()     # [B,L,T] CE from energies and target indices
â”‚   â”œâ”€â”€ find_path()           # DP to find minimal-cost path over CE (free energy)
â”‚   â”œâ”€â”€ path_cross_entropy()  # Sum along the DP path (the training loss)
â”‚   â”œâ”€â”€ decode_segmented()    # DP segmentation + per-segment letter choice (inference)
â”‚   â”œâ”€â”€ evaluate_char_acc()   # Char accuracy on a loader using decode_segmented
â”‚   â”œâ”€â”€ TrainLogger           # CSV logger + simple PNG plot (no pandas dependency)
â”‚   â””â”€â”€ save_config()         # Dump run config to outputs/config.json
â”œâ”€â”€ model.py          # SimpleNet CNN
â”œâ”€â”€ data.py           # SimpleWordsDataset: renders synthetic word images
â”œâ”€â”€ utils.py          # transform_word() + collate_fn() (returns images, targets, raw_texts)
â”œâ”€â”€ inference.py      # plot utilities: energy heatmap + path (for gold targets)
â”œâ”€â”€ config.py         # token constants (ALPHABET_SIZE, BETWEEN, DASH) + FONT_PATH
â”œâ”€â”€ outputs/          # logs, plots, checkpoints
â””â”€â”€ requirements.txt  # dependencies
```
